WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Reacher2Dof-v0 ==========
Seed: 1
OrderedDict([('n_timesteps', 1000000.0), ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:216: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:63: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:196: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:267: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:311: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_2
/home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f929d9dd1d0> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f92a40dbba8>
  "{} != {}".format(self.training_env, self.eval_env))
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.8601105  |
| ent_coef_loss           | -0.480593  |
| entropy                 | 2.3458204  |
| ep_rewmean              | -18.9      |
| episodes                | 4          |
| eplenmean               | 150        |
| fps                     | 452        |
| mean 100 episode reward | -18.9      |
| n_updates               | 501        |
| policy_loss             | -5.0400105 |
| qf1_loss                | 7.420905   |
| qf2_loss                | 8.161804   |
| success rate            | 0          |
| time_elapsed            | 1          |
| total timesteps         | 600        |
| value_loss              | 0.32157716 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.71878535  |
| ent_coef_loss           | -1.0728962  |
| entropy                 | 2.5725431   |
| ep_rewmean              | -17         |
| episodes                | 8           |
| eplenmean               | 150         |
| fps                     | 453         |
| mean 100 episode reward | -17         |
| n_updates               | 1101        |
| policy_loss             | -7.502445   |
| qf1_loss                | 2.4387822   |
| qf2_loss                | 2.378807    |
| success rate            | 0           |
| time_elapsed            | 2           |
| total timesteps         | 1200        |
| value_loss              | 0.110234424 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.6009758  |
| ent_coef_loss           | -1.6373943 |
| entropy                 | 2.6547189  |
| ep_rewmean              | -16.5      |
| episodes                | 12         |
| eplenmean               | 150        |
| fps                     | 444        |
| mean 100 episode reward | -16.5      |
| n_updates               | 1701       |
| policy_loss             | -9.304467  |
| qf1_loss                | 24.53717   |
| qf2_loss                | 25.23602   |
| success rate            | 0          |
| time_elapsed            | 4          |
| total timesteps         | 1800       |
| value_loss              | 0.12454423 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5032425  |
| ent_coef_loss           | -2.2164288 |
| entropy                 | 2.6762934  |
| ep_rewmean              | -16.6      |
| episodes                | 16         |
| eplenmean               | 150        |
| fps                     | 444        |
| mean 100 episode reward | -16.6      |
| n_updates               | 2301       |
| policy_loss             | -9.539719  |
| qf1_loss                | 3.8778195  |
| qf2_loss                | 3.98807    |
| success rate            | 0          |
| time_elapsed            | 5          |
| total timesteps         | 2400       |
| value_loss              | 0.21739687 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.42296046 |
| ent_coef_loss           | -2.7297082 |
| entropy                 | 2.589507   |
| ep_rewmean              | -17.1      |
| episodes                | 20         |
| eplenmean               | 150        |
| fps                     | 440        |
| mean 100 episode reward | -17.1      |
| n_updates               | 2901       |
| policy_loss             | -9.778769  |
| qf1_loss                | 0.5834794  |
| qf2_loss                | 0.63333607 |
| success rate            | 0          |
| time_elapsed            | 6          |
| total timesteps         | 3000       |
| value_loss              | 0.20848534 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.35705656 |
| ent_coef_loss           | -2.9466085 |
| entropy                 | 2.6525583  |
| ep_rewmean              | -16.5      |
| episodes                | 24         |
| eplenmean               | 150        |
| fps                     | 439        |
| mean 100 episode reward | -16.5      |
| n_updates               | 3501       |
| policy_loss             | -10.14443  |
| qf1_loss                | 1.4189057  |
| qf2_loss                | 1.5647953  |
| success rate            | 0          |
| time_elapsed            | 8          |
| total timesteps         | 3600       |
| value_loss              | 0.23150414 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.3026689  |
| ent_coef_loss           | -2.8765564 |
| entropy                 | 2.4937873  |
| ep_rewmean              | -15.8      |
| episodes                | 28         |
| eplenmean               | 150        |
| fps                     | 442        |
| mean 100 episode reward | -15.8      |
| n_updates               | 4101       |
| policy_loss             | -10.604468 |
| qf1_loss                | 1.1355426  |
| qf2_loss                | 1.1898     |
| success rate            | 0          |
| time_elapsed            | 9          |
| total timesteps         | 4200       |
| value_loss              | 0.15616632 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.25752458 |
| ent_coef_loss           | -3.2583647 |
| entropy                 | 2.4552603  |
| ep_rewmean              | -16.6      |
| episodes                | 32         |
| eplenmean               | 150        |
| fps                     | 444        |
| mean 100 episode reward | -16.6      |
| n_updates               | 4701       |
| policy_loss             | -10.080641 |
| qf1_loss                | 1.1158206  |
| qf2_loss                | 1.3090143  |
| success rate            | 0          |
| time_elapsed            | 10         |
| total timesteps         | 4800       |
| value_loss              | 0.19354555 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.21937254 |
| ent_coef_loss           | -3.3852313 |
| entropy                 | 2.3710194  |
| ep_rewmean              | -20.1      |
| episodes                | 36         |
| eplenmean               | 150        |
| fps                     | 444        |
| mean 100 episode reward | -20.1      |
| n_updates               | 5301       |
| policy_loss             | -10.228441 |
| qf1_loss                | 2.306871   |
| qf2_loss                | 2.8239229  |
| success rate            | 0          |
| time_elapsed            | 12         |
| total timesteps         | 5400       |
| value_loss              | 0.2939114  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.18608552 |
| ent_coef_loss           | -3.8724067 |
| entropy                 | 2.3713799  |
| ep_rewmean              | -19        |
| episodes                | 40         |
| eplenmean               | 150        |
| fps                     | 445        |
| mean 100 episode reward | -19        |
| n_updates               | 5901       |
| policy_loss             | -9.341676  |
| qf1_loss                | 2.4933076  |
| qf2_loss                | 2.5683305  |
| success rate            | 0          |
| time_elapsed            | 13         |
| total timesteps         | 6000       |
| value_loss              | 0.32212067 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.15868239 |
| ent_coef_loss           | -3.605454  |
| entropy                 | 2.38671    |
| ep_rewmean              | -18.3      |
| episodes                | 44         |
| eplenmean               | 150        |
| fps                     | 441        |
| mean 100 episode reward | -18.3      |
| n_updates               | 6501       |
| policy_loss             | -9.335861  |
| qf1_loss                | 2.7953575  |
| qf2_loss                | 2.840992   |
| success rate            | 0          |
| time_elapsed            | 14         |
| total timesteps         | 6600       |
| value_loss              | 0.13288209 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.13546537 |
| ent_coef_loss           | -3.135314  |
| entropy                 | 2.1022882  |
| ep_rewmean              | -18.2      |
| episodes                | 48         |
| eplenmean               | 150        |
| fps                     | 443        |
| mean 100 episode reward | -18.2      |
| n_updates               | 7101       |
| policy_loss             | -9.778177  |
| qf1_loss                | 3.0118694  |
| qf2_loss                | 3.4936087  |
| success rate            | 0          |
| time_elapsed            | 16         |
| total timesteps         | 7200       |
| value_loss              | 0.2708232  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.11582096 |
| ent_coef_loss           | -3.7310345 |
| entropy                 | 2.0926023  |
| ep_rewmean              | -17.4      |
| episodes                | 52         |
| eplenmean               | 150        |
| fps                     | 443        |
| mean 100 episode reward | -17.4      |
| n_updates               | 7701       |
| policy_loss             | -8.487753  |
| qf1_loss                | 0.5160918  |
| qf2_loss                | 0.4488044  |
| success rate            | 0.0192     |
| time_elapsed            | 17         |
| total timesteps         | 7800       |
| value_loss              | 0.1619201  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.09961277 |
| ent_coef_loss           | -2.9401414 |
| entropy                 | 2.2999346  |
| ep_rewmean              | -16.5      |
| episodes                | 56         |
| eplenmean               | 150        |
| fps                     | 444        |
| mean 100 episode reward | -16.5      |
| n_updates               | 8301       |
| policy_loss             | -7.9938116 |
| qf1_loss                | 0.37082556 |
| qf2_loss                | 0.38755006 |
| success rate            | 0.0179     |
| time_elapsed            | 18         |
| total timesteps         | 8400       |
| value_loss              | 0.11537806 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.085989684 |
| ent_coef_loss           | -3.706202   |
| entropy                 | 2.249868    |
| ep_rewmean              | -15.9       |
| episodes                | 60          |
| eplenmean               | 150         |
| fps                     | 443         |
| mean 100 episode reward | -15.9       |
| n_updates               | 8901        |
| policy_loss             | -7.656855   |
| qf1_loss                | 0.63078296  |
| qf2_loss                | 0.7039214   |
| success rate            | 0.0167      |
| time_elapsed            | 20          |
| total timesteps         | 9000        |
| value_loss              | 0.37255812  |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.075170815 |
| ent_coef_loss           | -2.7494652  |
| entropy                 | 2.1484613   |
| ep_rewmean              | -14.8       |
| episodes                | 64          |
| eplenmean               | 150         |
| fps                     | 441         |
| mean 100 episode reward | -14.8       |
| n_updates               | 9501        |
| policy_loss             | -7.1211777  |
| qf1_loss                | 0.32256255  |
| qf2_loss                | 0.27923635  |
| success rate            | 0.0156      |
| time_elapsed            | 21          |
| total timesteps         | 9600        |
| value_loss              | 0.2117812   |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-5.73 +/- 9.71
Episode length: 150.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_2
pybullet build time: Sep  9 2020 17:03:46
