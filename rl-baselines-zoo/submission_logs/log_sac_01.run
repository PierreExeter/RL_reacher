WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Reacher2Dof-v0 ==========
Seed: 1
OrderedDict([('n_timesteps', 1000000.0), ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:216: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:63: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:196: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:267: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:311: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_2
/home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f14f8056358> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f14f8051f60>
  "{} != {}".format(self.training_env, self.eval_env))
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.8599149  |
| ent_coef_loss           | -0.4960988 |
| entropy                 | 2.63008    |
| ep_rewmean              | -9.05      |
| episodes                | 4          |
| eplenmean               | 150        |
| fps                     | 440        |
| mean 100 episode reward | -9.1       |
| n_updates               | 501        |
| policy_loss             | -2.6802838 |
| qf1_loss                | 0.6906327  |
| qf2_loss                | 0.73787856 |
| success rate            | 0          |
| time_elapsed            | 1          |
| total timesteps         | 600        |
| value_loss              | 0.06920499 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.7187849   |
| ent_coef_loss           | -1.0770825  |
| entropy                 | 2.591275    |
| ep_rewmean              | -9.95       |
| episodes                | 8           |
| eplenmean               | 150         |
| fps                     | 434         |
| mean 100 episode reward | -10         |
| n_updates               | 1101        |
| policy_loss             | -4.9935503  |
| qf1_loss                | 0.9342777   |
| qf2_loss                | 0.92058754  |
| success rate            | 0           |
| time_elapsed            | 2           |
| total timesteps         | 1200        |
| value_loss              | 0.114152886 |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.6019957   |
| ent_coef_loss           | -1.5847497  |
| entropy                 | 2.5522757   |
| ep_rewmean              | -8.42       |
| episodes                | 12          |
| eplenmean               | 150         |
| fps                     | 444         |
| mean 100 episode reward | -8.4        |
| n_updates               | 1701        |
| policy_loss             | -6.8989897  |
| qf1_loss                | 1.7308505   |
| qf2_loss                | 1.7294413   |
| success rate            | 0           |
| time_elapsed            | 4           |
| total timesteps         | 1800        |
| value_loss              | 0.113070935 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5059013  |
| ent_coef_loss           | -2.0772953 |
| entropy                 | 2.5244043  |
| ep_rewmean              | -11.4      |
| episodes                | 16         |
| eplenmean               | 150        |
| fps                     | 450        |
| mean 100 episode reward | -11.4      |
| n_updates               | 2301       |
| policy_loss             | -8.460806  |
| qf1_loss                | 0.6413227  |
| qf2_loss                | 0.62620723 |
| success rate            | 0          |
| time_elapsed            | 5          |
| total timesteps         | 2400       |
| value_loss              | 0.2504524  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.42703837 |
| ent_coef_loss           | -2.529117  |
| entropy                 | 2.600119   |
| ep_rewmean              | -19.2      |
| episodes                | 20         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -19.2      |
| n_updates               | 2901       |
| policy_loss             | -10.331188 |
| qf1_loss                | 10.476784  |
| qf2_loss                | 10.840212  |
| success rate            | 0          |
| time_elapsed            | 6          |
| total timesteps         | 3000       |
| value_loss              | 0.55898726 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.35904709 |
| ent_coef_loss           | -3.265986  |
| entropy                 | 2.5224676  |
| ep_rewmean              | -19.2      |
| episodes                | 24         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -19.2      |
| n_updates               | 3501       |
| policy_loss             | -11.108543 |
| qf1_loss                | 9.463921   |
| qf2_loss                | 9.457949   |
| success rate            | 0          |
| time_elapsed            | 7          |
| total timesteps         | 3600       |
| value_loss              | 0.24322738 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.30110607  |
| ent_coef_loss           | -3.8112245  |
| entropy                 | 2.5705967   |
| ep_rewmean              | -18.3       |
| episodes                | 28          |
| eplenmean               | 150         |
| fps                     | 453         |
| mean 100 episode reward | -18.3       |
| n_updates               | 4101        |
| policy_loss             | -10.235479  |
| qf1_loss                | 6.9821606   |
| qf2_loss                | 7.5965586   |
| success rate            | 0           |
| time_elapsed            | 9           |
| total timesteps         | 4200        |
| value_loss              | 0.122109495 |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.25204054  |
| ent_coef_loss           | -4.0337143  |
| entropy                 | 2.5243623   |
| ep_rewmean              | -17.2       |
| episodes                | 32          |
| eplenmean               | 150         |
| fps                     | 453         |
| mean 100 episode reward | -17.2       |
| n_updates               | 4701        |
| policy_loss             | -10.316259  |
| qf1_loss                | 0.37246642  |
| qf2_loss                | 0.28197098  |
| success rate            | 0           |
| time_elapsed            | 10          |
| total timesteps         | 4800        |
| value_loss              | 0.113774285 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2107992  |
| ent_coef_loss           | -4.325163  |
| entropy                 | 2.5292718  |
| ep_rewmean              | -17.2      |
| episodes                | 36         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -17.2      |
| n_updates               | 5301       |
| policy_loss             | -10.095348 |
| qf1_loss                | 7.1340275  |
| qf2_loss                | 7.24506    |
| success rate            | 0          |
| time_elapsed            | 11         |
| total timesteps         | 5400       |
| value_loss              | 0.12115772 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.17656595  |
| ent_coef_loss           | -4.7587996  |
| entropy                 | 2.4936016   |
| ep_rewmean              | -16.8       |
| episodes                | 40          |
| eplenmean               | 150         |
| fps                     | 453         |
| mean 100 episode reward | -16.8       |
| n_updates               | 5901        |
| policy_loss             | -9.98373    |
| qf1_loss                | 0.18674232  |
| qf2_loss                | 0.15026328  |
| success rate            | 0           |
| time_elapsed            | 13          |
| total timesteps         | 6000        |
| value_loss              | 0.059765466 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.14802372 |
| ent_coef_loss           | -5.4292088 |
| entropy                 | 2.338849   |
| ep_rewmean              | -16.3      |
| episodes                | 44         |
| eplenmean               | 150        |
| fps                     | 454        |
| mean 100 episode reward | -16.3      |
| n_updates               | 6501       |
| policy_loss             | -8.146644  |
| qf1_loss                | 0.596298   |
| qf2_loss                | 0.51480335 |
| success rate            | 0          |
| time_elapsed            | 14         |
| total timesteps         | 6600       |
| value_loss              | 0.07829055 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.1244929  |
| ent_coef_loss           | -5.3038445 |
| entropy                 | 2.3205092  |
| ep_rewmean              | -16.3      |
| episodes                | 48         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -16.3      |
| n_updates               | 7101       |
| policy_loss             | -8.21384   |
| qf1_loss                | 7.4755497  |
| qf2_loss                | 7.664598   |
| success rate            | 0          |
| time_elapsed            | 15         |
| total timesteps         | 7200       |
| value_loss              | 0.07166207 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.10507338  |
| ent_coef_loss           | -6.9440556  |
| entropy                 | 2.309813    |
| ep_rewmean              | -16         |
| episodes                | 52          |
| eplenmean               | 150         |
| fps                     | 454         |
| mean 100 episode reward | -16         |
| n_updates               | 7701        |
| policy_loss             | -7.30772    |
| qf1_loss                | 0.47278303  |
| qf2_loss                | 0.42422622  |
| success rate            | 0           |
| time_elapsed            | 17          |
| total timesteps         | 7800        |
| value_loss              | 0.081734955 |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.088723816 |
| ent_coef_loss           | -6.452394   |
| entropy                 | 2.0829265   |
| ep_rewmean              | -15.6       |
| episodes                | 56          |
| eplenmean               | 150         |
| fps                     | 453         |
| mean 100 episode reward | -15.6       |
| n_updates               | 8301        |
| policy_loss             | -8.039764   |
| qf1_loss                | 0.80943954  |
| qf2_loss                | 0.6068951   |
| success rate            | 0           |
| time_elapsed            | 18          |
| total timesteps         | 8400        |
| value_loss              | 0.09249246  |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.07566465  |
| ent_coef_loss           | -5.390505   |
| entropy                 | 1.9487951   |
| ep_rewmean              | -15.4       |
| episodes                | 60          |
| eplenmean               | 150         |
| fps                     | 454         |
| mean 100 episode reward | -15.4       |
| n_updates               | 8901        |
| policy_loss             | -7.008198   |
| qf1_loss                | 0.5884716   |
| qf2_loss                | 0.57028097  |
| success rate            | 0           |
| time_elapsed            | 19          |
| total timesteps         | 9000        |
| value_loss              | 0.058132224 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.06467125 |
| ent_coef_loss           | -5.032503  |
| entropy                 | 1.8607488  |
| ep_rewmean              | -15        |
| episodes                | 64         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -15        |
| n_updates               | 9501       |
| policy_loss             | -6.4669547 |
| qf1_loss                | 0.39521283 |
| qf2_loss                | 0.39053398 |
| success rate            | 0          |
| time_elapsed            | 21         |
| total timesteps         | 9600       |
| value_loss              | 0.05322262 |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-6.57 +/- 11.74
Episode length: 150.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_2
pybullet build time: Sep  9 2020 17:03:46
