WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('n_timesteps', 1000000.0), ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=100000
Creating test environment
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:332: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/policies.py:134: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/policies.py:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:412: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:94: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:444: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:720: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:432: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/ddpg/ddpg.py:452: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/train_0.1M_Reacher2Dof-v0/ddpg/Reacher2Dof-v0_1
/home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<Monitor<TimeLimit<ReacherBulletEnv2<Reacher2Dof-v0>>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f636b2c4470>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-23.24 +/- 19.27
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -1.8     |
| reference_Q_std         | 6.23     |
| reference_action_mean   | 0.113    |
| reference_action_std    | 0.61     |
| reference_actor_Q_mean  | -1.38    |
| reference_actor_Q_std   | 6.07     |
| rollout/Q_mean          | -0.849   |
| rollout/actions_mean    | -0.282   |
| rollout/actions_std     | 0.822    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 66       |
| rollout/return          | -72.6    |
| rollout/return_history  | -72.6    |
| success rate            | 0        |
| total/duration          | 16.8     |
| total/episodes          | 66       |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 595      |
| train/loss_actor        | 1.33     |
| train/loss_critic       | 3.98     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=20000, episode_reward=3.92 +/- 5.02
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -1.21    |
| reference_Q_std         | 6.53     |
| reference_action_mean   | -0.263   |
| reference_action_std    | 0.855    |
| reference_actor_Q_mean  | -0.454   |
| reference_actor_Q_std   | 6.45     |
| rollout/Q_mean          | -0.697   |
| rollout/actions_mean    | -0.147   |
| rollout/actions_std     | 0.796    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 133      |
| rollout/return          | -56.6    |
| rollout/return_history  | -46      |
| success rate            | 0        |
| total/duration          | 34.5     |
| total/episodes          | 133      |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 580      |
| train/loss_actor        | 0.694    |
| train/loss_critic       | 1.62     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=30000, episode_reward=-10.67 +/- 8.19
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.36     |
| reference_Q_std         | 6.6      |
| reference_action_mean   | -0.247   |
| reference_action_std    | 0.843    |
| reference_actor_Q_mean  | 1.06     |
| reference_actor_Q_std   | 6.42     |
| rollout/Q_mean          | -0.0982  |
| rollout/actions_mean    | -0.218   |
| rollout/actions_std     | 0.753    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 200      |
| rollout/return          | -41.1    |
| rollout/return_history  | -12.7    |
| success rate            | 0        |
| total/duration          | 51.6     |
| total/episodes          | 200      |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 581      |
| train/loss_actor        | -0.505   |
| train/loss_critic       | 0.821    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=40000, episode_reward=-9.96 +/- 14.21
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.342    |
| reference_Q_std         | 6.69     |
| reference_action_mean   | 0.0123   |
| reference_action_std    | 0.769    |
| reference_actor_Q_mean  | 0.763    |
| reference_actor_Q_std   | 6.67     |
| rollout/Q_mean          | 0.378    |
| rollout/actions_mean    | -0.237   |
| rollout/actions_std     | 0.727    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 266      |
| rollout/return          | -31.8    |
| rollout/return_history  | -4.34    |
| success rate            | 0        |
| total/duration          | 69       |
| total/episodes          | 266      |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 580      |
| train/loss_actor        | -0.735   |
| train/loss_critic       | 0.684    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=50000, episode_reward=2.22 +/- 5.11
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.275   |
| reference_Q_std         | 6.44     |
| reference_action_mean   | -0.0105  |
| reference_action_std    | 0.826    |
| reference_actor_Q_mean  | 0.0983   |
| reference_actor_Q_std   | 6.37     |
| rollout/Q_mean          | 0.815    |
| rollout/actions_mean    | -0.206   |
| rollout/actions_std     | 0.707    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 333      |
| rollout/return          | -25.9    |
| rollout/return_history  | -2.76    |
| success rate            | 0        |
| total/duration          | 85.9     |
| total/episodes          | 333      |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 582      |
| train/loss_actor        | -1.13    |
| train/loss_critic       | 0.459    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=60000, episode_reward=0.24 +/- 13.65
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.542   |
| reference_Q_std         | 6.44     |
| reference_action_mean   | -0.0813  |
| reference_action_std    | 0.856    |
| reference_actor_Q_mean  | -0.28    |
| reference_actor_Q_std   | 6.39     |
| rollout/Q_mean          | 1.09     |
| rollout/actions_mean    | -0.21    |
| rollout/actions_std     | 0.69     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 400      |
| rollout/return          | -21.5    |
| rollout/return_history  | -0.998   |
| success rate            | 0        |
| total/duration          | 103      |
| total/episodes          | 400      |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 585      |
| train/loss_actor        | -1.88    |
| train/loss_critic       | 0.255    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=70000, episode_reward=7.94 +/- 3.25
Episode length: 150.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -0.492   |
| reference_Q_std         | 6.64     |
| reference_action_mean   | 0.0764   |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 0.123    |
| reference_actor_Q_std   | 6.61     |
| rollout/Q_mean          | 1.39     |
| rollout/actions_mean    | -0.205   |
| rollout/actions_std     | 0.682    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 466      |
| rollout/return          | -18.6    |
| rollout/return_history  | -0.361   |
| success rate            | 0.02     |
| total/duration          | 119      |
| total/episodes          | 466      |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 586      |
| train/loss_actor        | -2.58    |
| train/loss_critic       | 0.154    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=80000, episode_reward=-13.00 +/- 13.24
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.28    |
| reference_Q_std         | 7.77     |
| reference_action_mean   | 0.247    |
| reference_action_std    | 0.861    |
| reference_actor_Q_mean  | -0.639   |
| reference_actor_Q_std   | 7.59     |
| rollout/Q_mean          | 1.59     |
| rollout/actions_mean    | -0.206   |
| rollout/actions_std     | 0.672    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 533      |
| rollout/return          | -16.4    |
| rollout/return_history  | 0.455    |
| success rate            | 0        |
| total/duration          | 136      |
| total/episodes          | 533      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 587      |
| train/loss_actor        | -2.77    |
| train/loss_critic       | 0.122    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=90000, episode_reward=-2.14 +/- 12.03
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.33    |
| reference_Q_std         | 7.89     |
| reference_action_mean   | 0.241    |
| reference_action_std    | 0.873    |
| reference_actor_Q_mean  | -0.57    |
| reference_actor_Q_std   | 7.58     |
| rollout/Q_mean          | 1.77     |
| rollout/actions_mean    | -0.198   |
| rollout/actions_std     | 0.67     |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 600      |
| rollout/return          | -15      |
| rollout/return_history  | -2.8     |
| success rate            | 0        |
| total/duration          | 153      |
| total/episodes          | 600      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 588      |
| train/loss_actor        | -2.92    |
| train/loss_critic       | 0.131    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=100000, episode_reward=-2.88 +/- 9.37
Episode length: 150.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.56    |
| reference_Q_std         | 8.02     |
| reference_action_mean   | 0.224    |
| reference_action_std    | 0.913    |
| reference_actor_Q_mean  | -0.827   |
| reference_actor_Q_std   | 7.75     |
| rollout/Q_mean          | 1.88     |
| rollout/actions_mean    | -0.202   |
| rollout/actions_std     | 0.663    |
| rollout/episode_steps   | 150      |
| rollout/episodes        | 666      |
| rollout/return          | -13.5    |
| rollout/return_history  | -1.55    |
| success rate            | 0        |
| total/duration          | 170      |
| total/episodes          | 666      |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 589      |
| train/loss_actor        | -2.99    |
| train/loss_critic       | 0.152    |
| train/param_noise_di... | 0        |
--------------------------------------

Saving to logs/train_0.1M_Reacher2Dof-v0/ddpg/Reacher2Dof-v0_1
pybullet build time: Sep  9 2020 17:03:46
