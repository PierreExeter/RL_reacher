WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('n_timesteps', 1000000.0), ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:216: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:63: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:196: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:267: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:311: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_1
/home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7faef2fe22e8> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7faef2fddf28>
  "{} != {}".format(self.training_env, self.eval_env))
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.8562631   |
| ent_coef_loss           | -0.51998353 |
| entropy                 | 2.5999427   |
| ep_rewmean              | -16.4       |
| episodes                | 4           |
| eplenmean               | 150         |
| fps                     | 451         |
| mean 100 episode reward | -16.4       |
| n_updates               | 501         |
| policy_loss             | -2.6946573  |
| qf1_loss                | 0.99950254  |
| qf2_loss                | 1.0067915   |
| success rate            | 0           |
| time_elapsed            | 1           |
| total timesteps         | 600         |
| value_loss              | 0.098956555 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.7147748  |
| ent_coef_loss           | -1.1373748 |
| entropy                 | 2.606988   |
| ep_rewmean              | -11.6      |
| episodes                | 8          |
| eplenmean               | 150        |
| fps                     | 470        |
| mean 100 episode reward | -11.6      |
| n_updates               | 1101       |
| policy_loss             | -4.7479396 |
| qf1_loss                | 0.9543105  |
| qf2_loss                | 0.94661754 |
| success rate            | 0          |
| time_elapsed            | 2          |
| total timesteps         | 1200       |
| value_loss              | 0.08412157 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5982303  |
| ent_coef_loss           | -1.6465808 |
| entropy                 | 2.5790725  |
| ep_rewmean              | -10.6      |
| episodes                | 12         |
| eplenmean               | 150        |
| fps                     | 456        |
| mean 100 episode reward | -10.6      |
| n_updates               | 1701       |
| policy_loss             | -6.3795547 |
| qf1_loss                | 0.43779624 |
| qf2_loss                | 0.5846264  |
| success rate            | 0          |
| time_elapsed            | 3          |
| total timesteps         | 1800       |
| value_loss              | 0.21944594 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.50358176 |
| ent_coef_loss           | -2.0210793 |
| entropy                 | 2.4754677  |
| ep_rewmean              | -14.1      |
| episodes                | 16         |
| eplenmean               | 150        |
| fps                     | 437        |
| mean 100 episode reward | -14.1      |
| n_updates               | 2301       |
| policy_loss             | -7.788741  |
| qf1_loss                | 0.4621146  |
| qf2_loss                | 0.45112145 |
| success rate            | 0          |
| time_elapsed            | 5          |
| total timesteps         | 2400       |
| value_loss              | 0.25311434 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.42641383 |
| ent_coef_loss           | -2.414733  |
| entropy                 | 2.4514723  |
| ep_rewmean              | -13.7      |
| episodes                | 20         |
| eplenmean               | 150        |
| fps                     | 425        |
| mean 100 episode reward | -13.7      |
| n_updates               | 2901       |
| policy_loss             | -8.788528  |
| qf1_loss                | 0.8447893  |
| qf2_loss                | 0.6952786  |
| success rate            | 0          |
| time_elapsed            | 7          |
| total timesteps         | 3000       |
| value_loss              | 0.26986074 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.36209854 |
| ent_coef_loss           | -2.9106522 |
| entropy                 | 2.3803089  |
| ep_rewmean              | -13.9      |
| episodes                | 24         |
| eplenmean               | 150        |
| fps                     | 418        |
| mean 100 episode reward | -13.9      |
| n_updates               | 3501       |
| policy_loss             | -9.091453  |
| qf1_loss                | 4.919993   |
| qf2_loss                | 4.9134054  |
| success rate            | 0          |
| time_elapsed            | 8          |
| total timesteps         | 3600       |
| value_loss              | 0.27399963 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.30767477 |
| ent_coef_loss           | -2.9666843 |
| entropy                 | 2.3535612  |
| ep_rewmean              | -13.3      |
| episodes                | 28         |
| eplenmean               | 150        |
| fps                     | 412        |
| mean 100 episode reward | -13.3      |
| n_updates               | 4101       |
| policy_loss             | -9.542072  |
| qf1_loss                | 1.6607828  |
| qf2_loss                | 1.6680951  |
| success rate            | 0          |
| time_elapsed            | 10         |
| total timesteps         | 4200       |
| value_loss              | 0.30441338 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2602555  |
| ent_coef_loss           | -3.5135903 |
| entropy                 | 2.399361   |
| ep_rewmean              | -14.2      |
| episodes                | 32         |
| eplenmean               | 150        |
| fps                     | 412        |
| mean 100 episode reward | -14.2      |
| n_updates               | 4701       |
| policy_loss             | -9.297841  |
| qf1_loss                | 0.66093874 |
| qf2_loss                | 0.54037654 |
| success rate            | 0          |
| time_elapsed            | 11         |
| total timesteps         | 4800       |
| value_loss              | 0.19222544 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.21914883 |
| ent_coef_loss           | -3.8372865 |
| entropy                 | 2.4314609  |
| ep_rewmean              | -14        |
| episodes                | 36         |
| eplenmean               | 150        |
| fps                     | 413        |
| mean 100 episode reward | -14        |
| n_updates               | 5301       |
| policy_loss             | -9.131105  |
| qf1_loss                | 1.3193958  |
| qf2_loss                | 0.9540011  |
| success rate            | 0          |
| time_elapsed            | 13         |
| total timesteps         | 5400       |
| value_loss              | 0.2884617  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.18423074 |
| ent_coef_loss           | -4.383076  |
| entropy                 | 2.403715   |
| ep_rewmean              | -14.5      |
| episodes                | 40         |
| eplenmean               | 150        |
| fps                     | 418        |
| mean 100 episode reward | -14.5      |
| n_updates               | 5901       |
| policy_loss             | -8.777546  |
| qf1_loss                | 1.2976396  |
| qf2_loss                | 1.2566491  |
| success rate            | 0          |
| time_elapsed            | 14         |
| total timesteps         | 6000       |
| value_loss              | 0.2878513  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.15522808 |
| ent_coef_loss           | -3.8925414 |
| entropy                 | 2.4768846  |
| ep_rewmean              | -14.9      |
| episodes                | 44         |
| eplenmean               | 150        |
| fps                     | 423        |
| mean 100 episode reward | -14.9      |
| n_updates               | 6501       |
| policy_loss             | -9.998511  |
| qf1_loss                | 1.338039   |
| qf2_loss                | 1.4823685  |
| success rate            | 0          |
| time_elapsed            | 15         |
| total timesteps         | 6600       |
| value_loss              | 0.16097577 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.1358114  |
| ent_coef_loss           | -3.1343474 |
| entropy                 | 2.2308125  |
| ep_rewmean              | -18.4      |
| episodes                | 48         |
| eplenmean               | 150        |
| fps                     | 426        |
| mean 100 episode reward | -18.4      |
| n_updates               | 7101       |
| policy_loss             | -10.978634 |
| qf1_loss                | 2.4239278  |
| qf2_loss                | 2.2718627  |
| success rate            | 0          |
| time_elapsed            | 16         |
| total timesteps         | 7200       |
| value_loss              | 0.16847464 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.12119402 |
| ent_coef_loss           | -3.177003  |
| entropy                 | 2.224688   |
| ep_rewmean              | -20        |
| episodes                | 52         |
| eplenmean               | 150        |
| fps                     | 428        |
| mean 100 episode reward | -20        |
| n_updates               | 7701       |
| policy_loss             | -10.255907 |
| qf1_loss                | 0.876003   |
| qf2_loss                | 0.9950759  |
| success rate            | 0          |
| time_elapsed            | 18         |
| total timesteps         | 7800       |
| value_loss              | 0.13643463 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.10427565 |
| ent_coef_loss           | -4.15801   |
| entropy                 | 2.1650128  |
| ep_rewmean              | -20.3      |
| episodes                | 56         |
| eplenmean               | 150        |
| fps                     | 431        |
| mean 100 episode reward | -20.3      |
| n_updates               | 8301       |
| policy_loss             | -11.351122 |
| qf1_loss                | 0.79165643 |
| qf2_loss                | 0.96973026 |
| success rate            | 0          |
| time_elapsed            | 19         |
| total timesteps         | 8400       |
| value_loss              | 0.1690782  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.08533414 |
| ent_coef_loss           | -5.050537  |
| entropy                 | 2.2432942  |
| ep_rewmean              | -20.6      |
| episodes                | 60         |
| eplenmean               | 150        |
| fps                     | 432        |
| mean 100 episode reward | -20.6      |
| n_updates               | 8901       |
| policy_loss             | -11.31098  |
| qf1_loss                | 1.016144   |
| qf2_loss                | 1.149736   |
| success rate            | 0          |
| time_elapsed            | 20         |
| total timesteps         | 9000       |
| value_loss              | 0.13441145 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.069574125 |
| ent_coef_loss           | -5.934374   |
| entropy                 | 2.3380404   |
| ep_rewmean              | -20.9       |
| episodes                | 64          |
| eplenmean               | 150         |
| fps                     | 429         |
| mean 100 episode reward | -20.9       |
| n_updates               | 9501        |
| policy_loss             | -9.142715   |
| qf1_loss                | 0.8293321   |
| qf2_loss                | 0.7950753   |
| success rate            | 0           |
| time_elapsed            | 22          |
| total timesteps         | 9600        |
| value_loss              | 0.055857003 |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-26.33 +/- 55.05
Episode length: 150.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_1
pybullet build time: Sep  9 2020 17:03:46
