WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

========== Reacher2Dof-v0 ==========
Seed: 0
OrderedDict([('n_timesteps', 1000000.0), ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:216: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/policies.py:63: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:196: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:267: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:311: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

Log path: logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_1
/home/pierre/bin/anaconda3/envs/reacher_link/lib/python3.7/site-packages/stable_baselines/common/callbacks.py:287: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f3837b47208> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3837b47048>
  "{} != {}".format(self.training_env, self.eval_env))
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.8591376   |
| ent_coef_loss           | -0.50779766 |
| entropy                 | 2.5558305   |
| ep_rewmean              | -12.9       |
| episodes                | 4           |
| eplenmean               | 150         |
| fps                     | 441         |
| mean 100 episode reward | -12.9       |
| n_updates               | 501         |
| policy_loss             | -3.3667953  |
| qf1_loss                | 0.40125912  |
| qf2_loss                | 0.40620404  |
| success rate            | 0           |
| time_elapsed            | 1           |
| total timesteps         | 600         |
| value_loss              | 0.05970612  |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.7179966  |
| ent_coef_loss           | -1.1111379 |
| entropy                 | 2.629094   |
| ep_rewmean              | -10.8      |
| episodes                | 8          |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -10.8      |
| n_updates               | 1101       |
| policy_loss             | -5.8527517 |
| qf1_loss                | 0.33104938 |
| qf2_loss                | 0.40294835 |
| success rate            | 0          |
| time_elapsed            | 2          |
| total timesteps         | 1200       |
| value_loss              | 0.15392703 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.603119   |
| ent_coef_loss           | -1.5451908 |
| entropy                 | 2.5757654  |
| ep_rewmean              | -8.91      |
| episodes                | 12         |
| eplenmean               | 150        |
| fps                     | 455        |
| mean 100 episode reward | -8.9       |
| n_updates               | 1701       |
| policy_loss             | -7.628463  |
| qf1_loss                | 1.218667   |
| qf2_loss                | 1.1222364  |
| success rate            | 0          |
| time_elapsed            | 3          |
| total timesteps         | 1800       |
| value_loss              | 0.22113384 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5101341  |
| ent_coef_loss           | -1.9667165 |
| entropy                 | 2.480628   |
| ep_rewmean              | -11.3      |
| episodes                | 16         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -11.3      |
| n_updates               | 2301       |
| policy_loss             | -10.562396 |
| qf1_loss                | 2.662795   |
| qf2_loss                | 2.3869972  |
| success rate            | 0          |
| time_elapsed            | 5          |
| total timesteps         | 2400       |
| value_loss              | 0.65467834 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.4353212  |
| ent_coef_loss           | -2.0345552 |
| entropy                 | 2.4727724  |
| ep_rewmean              | -12.7      |
| episodes                | 20         |
| eplenmean               | 150        |
| fps                     | 453        |
| mean 100 episode reward | -12.7      |
| n_updates               | 2901       |
| policy_loss             | -12.45043  |
| qf1_loss                | 1.3649645  |
| qf2_loss                | 1.4777578  |
| success rate            | 0          |
| time_elapsed            | 6          |
| total timesteps         | 3000       |
| value_loss              | 0.5489572  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.37244388 |
| ent_coef_loss           | -2.3569746 |
| entropy                 | 2.4379654  |
| ep_rewmean              | -12.7      |
| episodes                | 24         |
| eplenmean               | 150        |
| fps                     | 454        |
| mean 100 episode reward | -12.7      |
| n_updates               | 3501       |
| policy_loss             | -11.141987 |
| qf1_loss                | 1.2705033  |
| qf2_loss                | 1.2666299  |
| success rate            | 0          |
| time_elapsed            | 7          |
| total timesteps         | 3600       |
| value_loss              | 0.70375574 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.31674242 |
| ent_coef_loss           | -2.515809  |
| entropy                 | 2.3474889  |
| ep_rewmean              | -15.8      |
| episodes                | 28         |
| eplenmean               | 150        |
| fps                     | 452        |
| mean 100 episode reward | -15.8      |
| n_updates               | 4101       |
| policy_loss             | -11.49547  |
| qf1_loss                | 6.8268137  |
| qf2_loss                | 6.8851185  |
| success rate            | 0          |
| time_elapsed            | 9          |
| total timesteps         | 4200       |
| value_loss              | 0.91711175 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.26658374 |
| ent_coef_loss           | -3.5092878 |
| entropy                 | 2.4459968  |
| ep_rewmean              | -16.3      |
| episodes                | 32         |
| eplenmean               | 150        |
| fps                     | 449        |
| mean 100 episode reward | -16.3      |
| n_updates               | 4701       |
| policy_loss             | -11.890356 |
| qf1_loss                | 0.9480657  |
| qf2_loss                | 1.0146964  |
| success rate            | 0          |
| time_elapsed            | 10         |
| total timesteps         | 4800       |
| value_loss              | 0.18992427 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.22341463  |
| ent_coef_loss           | -3.247758   |
| entropy                 | 2.4198406   |
| ep_rewmean              | -16.4       |
| episodes                | 36          |
| eplenmean               | 150         |
| fps                     | 450         |
| mean 100 episode reward | -16.4       |
| n_updates               | 5301        |
| policy_loss             | -13.5119705 |
| qf1_loss                | 9.487359    |
| qf2_loss                | 9.252523    |
| success rate            | 0           |
| time_elapsed            | 11          |
| total timesteps         | 5400        |
| value_loss              | 0.60194504  |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.18818589 |
| ent_coef_loss           | -3.2995243 |
| entropy                 | 2.4000275  |
| ep_rewmean              | -17        |
| episodes                | 40         |
| eplenmean               | 150        |
| fps                     | 450        |
| mean 100 episode reward | -17        |
| n_updates               | 5901       |
| policy_loss             | -12.743405 |
| qf1_loss                | 3.4437976  |
| qf2_loss                | 3.2124853  |
| success rate            | 0          |
| time_elapsed            | 13         |
| total timesteps         | 6000       |
| value_loss              | 0.29397428 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.16028333 |
| ent_coef_loss           | -4.427082  |
| entropy                 | 2.4453104  |
| ep_rewmean              | -17.9      |
| episodes                | 44         |
| eplenmean               | 150        |
| fps                     | 450        |
| mean 100 episode reward | -17.9      |
| n_updates               | 6501       |
| policy_loss             | -12.869888 |
| qf1_loss                | 0.64265573 |
| qf2_loss                | 0.73590106 |
| success rate            | 0          |
| time_elapsed            | 14         |
| total timesteps         | 6600       |
| value_loss              | 0.28987622 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.13920535 |
| ent_coef_loss           | -1.819247  |
| entropy                 | 2.202248   |
| ep_rewmean              | -21        |
| episodes                | 48         |
| eplenmean               | 150        |
| fps                     | 451        |
| mean 100 episode reward | -21        |
| n_updates               | 7101       |
| policy_loss             | -13.663333 |
| qf1_loss                | 1.9084644  |
| qf2_loss                | 1.7471468  |
| success rate            | 0          |
| time_elapsed            | 15         |
| total timesteps         | 7200       |
| value_loss              | 0.19927642 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.12231466 |
| ent_coef_loss           | -2.949378  |
| entropy                 | 2.1531     |
| ep_rewmean              | -23        |
| episodes                | 52         |
| eplenmean               | 150        |
| fps                     | 452        |
| mean 100 episode reward | -23        |
| n_updates               | 7701       |
| policy_loss             | -12.513937 |
| qf1_loss                | 0.8875022  |
| qf2_loss                | 0.97179717 |
| success rate            | 0          |
| time_elapsed            | 17         |
| total timesteps         | 7800       |
| value_loss              | 0.2349877  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.10568747 |
| ent_coef_loss           | -2.7231221 |
| entropy                 | 2.20361    |
| ep_rewmean              | -24.5      |
| episodes                | 56         |
| eplenmean               | 150        |
| fps                     | 452        |
| mean 100 episode reward | -24.5      |
| n_updates               | 8301       |
| policy_loss             | -13.950462 |
| qf1_loss                | 1.6200833  |
| qf2_loss                | 1.5915066  |
| success rate            | 0          |
| time_elapsed            | 18         |
| total timesteps         | 8400       |
| value_loss              | 0.14724272 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.08902878 |
| ent_coef_loss           | -3.113461  |
| entropy                 | 2.0796595  |
| ep_rewmean              | -25.4      |
| episodes                | 60         |
| eplenmean               | 150        |
| fps                     | 451        |
| mean 100 episode reward | -25.4      |
| n_updates               | 8901       |
| policy_loss             | -11.421019 |
| qf1_loss                | 1.3269762  |
| qf2_loss                | 1.5522037  |
| success rate            | 0          |
| time_elapsed            | 19         |
| total timesteps         | 9000       |
| value_loss              | 0.1972706  |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.073767394 |
| ent_coef_loss           | -4.7983904  |
| entropy                 | 2.216707    |
| ep_rewmean              | -25         |
| episodes                | 64          |
| eplenmean               | 150         |
| fps                     | 450         |
| mean 100 episode reward | -25         |
| n_updates               | 9501        |
| policy_loss             | -12.191446  |
| qf1_loss                | 1.4817282   |
| qf2_loss                | 1.4997096   |
| success rate            | 0           |
| time_elapsed            | 21          |
| total timesteps         | 9600        |
| value_loss              | 0.14057004  |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=5.77 +/- 6.56
Episode length: 150.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_Reacher2Dof-v0/sac/Reacher2Dof-v0_1
pybullet build time: Sep  9 2020 17:03:46
